{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a4070c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac32638",
   "metadata": {
    "papermill": {
     "duration": 0.00152,
     "end_time": "2024-12-26T17:20:43.792059",
     "exception": false,
     "start_time": "2024-12-26T17:20:43.790539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Benchmark \n",
    "---\n",
    "\n",
    "This notebook compares the decoder performance between Viterbi Decoder and Neural Decoder [1] on Convolution Codes over AWGN Channel.\n",
    "\n",
    "Reference:\n",
    "* [1] Kim, Hyeji, et al. \"Communication Algorithms via Deep Learning.\" ICLR (2018)\n",
    "\n",
    "\n",
    "### TODOs:\n",
    "---\n",
    "\n",
    "* [x] Benchmark Viterbi Decoder \n",
    "* [x] Benchmark Neural Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f3a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T17:20:43.795240Z",
     "iopub.status.busy": "2024-12-26T17:20:43.794763Z",
     "iopub.status.idle": "2024-12-26T17:20:43.799478Z",
     "shell.execute_reply": "2024-12-26T17:20:43.799007Z"
    },
    "papermill": {
     "duration": 0.006817,
     "end_time": "2024-12-26T17:20:43.800064",
     "exception": false,
     "start_time": "2024-12-26T17:20:43.793247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "# a hack to import module from different directory\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd05cc37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T17:20:43.802546Z",
     "iopub.status.busy": "2024-12-26T17:20:43.802407Z",
     "iopub.status.idle": "2024-12-26T17:20:46.246941Z",
     "shell.execute_reply": "2024-12-26T17:20:46.246431Z"
    },
    "papermill": {
     "duration": 2.446713,
     "end_time": "2024-12-26T17:20:46.247798",
     "exception": false,
     "start_time": "2024-12-26T17:20:43.801085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 22:50:44.725302: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-26 22:50:44.943245: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 22:50:45.591197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import commpy as cp\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "\n",
    "from deepcom.metrics import BER, BLER         # metrics to benchmark Neural Decoder Model\n",
    "from deepcom.utils import corrupt_signal      # simulate a AWGN Channel\n",
    "from deepcom.dataset import data_genenerator  # data loader for Tensorflow\n",
    "from deepcom.model import NRSCDecoder # Neural Decoder Model\n",
    "\n",
    "BATCH_SIZE = 500       # depends on size of GPU, should be a factor of number_testing_sequences\n",
    "BLOCK_LEN = 100        # length of a message bits\n",
    "CONSTRAINT_LEN = 3     # num of shifts in Conv. Encoder\n",
    "TRACE_BACK_DEPTH = 15  # (?) a parameter Viterbi Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe48bfd",
   "metadata": {
    "papermill": {
     "duration": 0.000903,
     "end_time": "2024-12-26T17:20:46.249956",
     "exception": false,
     "start_time": "2024-12-26T17:20:46.249053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Dataset\n",
    "* Dataset should be generated using script `generate_synthetic_dataset.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a528d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df571622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T17:20:46.252470Z",
     "iopub.status.busy": "2024-12-26T17:20:46.252199Z",
     "iopub.status.idle": "2024-12-26T17:20:46.628241Z",
     "shell.execute_reply": "2024-12-26T17:20:46.627576Z"
    },
    "papermill": {
     "duration": 0.378114,
     "end_time": "2024-12-26T17:20:46.628969",
     "exception": true,
     "start_time": "2024-12-26T17:20:46.250855",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../rnn_120k_bl100_snr0.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m DATASET_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../rnn_120k_bl100_snr0.dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     _, _, X_test, Y_test \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)  \u001b[38;5;66;03m# ignore training data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of testing sequences \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(X_test)))\n",
      "File \u001b[0;32m~/miniconda3/envs/turbo3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../rnn_120k_bl100_snr0.dataset'"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../rnn_120k_bl100_snr0.dataset'\n",
    "with open(DATASET_PATH, 'rb') as f:\n",
    "    _, _, X_test, Y_test = pickle.load(f)  # ignore training data\n",
    "print('Number of testing sequences {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d59b79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Load pre-trained Neural Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdffacb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_log = 'logs/BiGRU-2-400::dropout-0.3::epochs-50'\n",
    "try:\n",
    "    model_path = os.path.join(experiment_log, 'BiGRU.keras')\n",
    "    print(model_path)\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={'BER': BER, 'BLER': BLER})\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    print('Pre-trained model is loaded.')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise ValueError('Pre-trained model is not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792f78e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Setup Viterbi/Neural Decoder Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094d9a5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def benchmark_neural_decoder(noisy_inputs, labels):\n",
    "    \n",
    "    # Set up data generator\n",
    "    Y = np.reshape(labels, (-1, BLOCK_LEN, 1))\n",
    "    X = np.reshape(np.array(noisy_inputs)[:, :2*BLOCK_LEN], (-1, BLOCK_LEN, 2))\n",
    "    test_set = data_genenerator(X, Y, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Make predictions on batch\n",
    "    decoded_bits = model.predict(\n",
    "        test_set.make_one_shot_iterator(), \n",
    "        steps=len(Y) // BATCH_SIZE)\n",
    "    \n",
    "    # Compute hamming distances\n",
    "    original_bits = np.reshape(Y, (-1, BLOCK_LEN)).astype(int)\n",
    "    decoded_bits =  np.reshape(np.round(decoded_bits), (-1, BLOCK_LEN)).astype(int)\n",
    "    hamming_dist = np.not_equal(original_bits, decoded_bits)\n",
    "    \n",
    "    return np.sum(hamming_dist, axis=1)\n",
    "\n",
    "def benchmark_viterbi(message_bits, noisy_bits, sigma):\n",
    "  \n",
    "    # make fair comparison between (100, 204) convolutional code and RNN decoder\n",
    "    # Reference: Author's code\n",
    "    noisy_bits[-2*int(M):] = 0\n",
    "    \n",
    "    # Viterbi Decoder on Conv. Code\n",
    "    decoded_bits = cp.channelcoding.viterbi_decode(\n",
    "        coded_bits=noisy_bits.astype(float), \n",
    "        trellis=trellis,\n",
    "        tb_depth=TRACE_BACK_DEPTH,\n",
    "        decoding_type='unquantized')\n",
    "    \n",
    "    # Number of bit errors (hamming distance)\n",
    "    hamming_dist = cp.utilities.hamming_dist(\n",
    "        message_bits.astype(int),\n",
    "        decoded_bits[:-int(M)])\n",
    "    return hamming_dist\n",
    "\n",
    " \n",
    "from  commpy.channelcoding  import Trellis\n",
    "#  Generator Matrix (octal representation)\n",
    "G = np.array([[0o7, 0o5]]) \n",
    "M = np.array([CONSTRAINT_LEN - 1])\n",
    "trellis = Trellis(M, G, feedback=0o7, code_type='rsc')\n",
    "\n",
    "# #################################################################\n",
    "# For every SNR_db, we generates new noisy signals\n",
    "# for fair comparision.\n",
    "# #################################################################\n",
    "def generate_noisy_input(message_bits, trellis, sigma):\n",
    "    # Encode message bit\n",
    "    coded_bits = cp.channelcoding.conv_encode(message_bits, trellis)\n",
    "    # Corrupt message on BAWGN Channel\n",
    "    coded_bits = corrupt_signal(coded_bits, noise_type='awgn', sigma=sigma)\n",
    "    return coded_bits, message_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cab00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "viterbiBERs, viterbiBLERs = [], []\n",
    "neuralBERs, neuralBLERs = [], []\n",
    "\n",
    "pool = mp.Pool(processes=mp.cpu_count())\n",
    "labels = np.reshape(Y_test, (-1, BLOCK_LEN)).astype(int)\n",
    "try: \n",
    "    SNRs  = np.linspace(0, 7.0, 8)\n",
    "    for snr in SNRs:\n",
    "        snr_linear = snr + 10 * np.log10(1./2.)\n",
    "        sigma = np.sqrt(1. / (2. * 10 **(snr_linear / 10.)))\n",
    "        print('[SNR]={:.2f}'.format(snr))\n",
    "        \n",
    "        # Generates new noisy signals\n",
    "        result = pool.starmap(\n",
    "            func=generate_noisy_input,  \n",
    "            iterable=[(msg_bits, trellis, sigma) for msg_bits in labels])\n",
    "        \n",
    "        X, Y =  zip(*result)\n",
    "        \n",
    "        # #################################################################\n",
    "        # BENCHMARK NEURAL DECODER \n",
    "        # #################################################################\n",
    "        nn_start = time.time()\n",
    "        hamm_dists = benchmark_neural_decoder(X, Y)\n",
    "        \n",
    "        nn_ber = sum(hamm_dists) / np.product(np.shape(Y))\n",
    "        nn_bler = np.count_nonzero(hamm_dists) / len(Y)\n",
    "        \n",
    "        neuralBERs.append(nn_ber)\n",
    "        neuralBLERs.append(nn_bler)            \n",
    "        print('\\tNeural Decoder:  [BER]={:5.7f} [BLER]={:5.3f} -- {:3.3f}s'.format(\n",
    "            nn_ber, nn_bler, time.time() - nn_start)) \n",
    "\n",
    "        # #################################################################\n",
    "        # BENCHMARK VITERBI DECODER \n",
    "        # #################################################################\n",
    "        vi_start = time.time()\n",
    "        hamm_dists = pool.starmap(benchmark_viterbi, [(y, x, sigma) for x, y in zip(X, Y)])\n",
    "        \n",
    "        ber = sum(hamm_dists) / np.product(np.shape(Y))\n",
    "        bler = np.count_nonzero(hamm_dists) / len(Y)\n",
    "        \n",
    "        viterbiBERs.append(ber)\n",
    "        viterbiBLERs.append(bler)\n",
    "        print('\\tViterbi Decoder: [BER]={:5.7f} [BLER]={:5.3f} -- {:3.3f}s'.format(\n",
    "              ber, bler, time.time() - vi_start))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d41550",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# ###################################\n",
    "# Plot Bit Error Rate (BER) Curve\n",
    "# ###################################\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Block Length = 100 || Data rate = 1/2', fontsize=14)\n",
    "\n",
    "plt.semilogy(SNRs, neuralBERs, '-vr')\n",
    "plt.semilogy(SNRs, viterbiBERs, 's--')\n",
    "plt.legend(['N-RSC (SNR_train=0 dB)', 'Viterbi'], fontsize=16)\n",
    "plt.xlabel('SNR', fontsize=16)\n",
    "plt.xlim(xmin=SNRs[0], xmax=SNRs[-1])  # this line\n",
    "plt.ylabel('BER', fontsize=16)\n",
    "plt.grid(True, which='both')\n",
    "plt.savefig('result_ber_block_length_1000_snr0.png')\n",
    "\n",
    "# ###################################\n",
    "# Plot Block Error Rate (BLER) Curve\n",
    "# ###################################\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Block Length = 100 || Data rate = 1/2', fontsize=14)\n",
    "\n",
    "plt.semilogy(SNRs, neuralBLERs, '-vr')\n",
    "plt.semilogy(SNRs, viterbiBLERs, 's--')\n",
    "plt.ylabel('BLER', fontsize=16)\n",
    "plt.xlabel('SNR', fontsize=16)\n",
    "plt.legend(['N-RSC (SNR_train=0 dB)', 'Viterbi'], fontsize=16)\n",
    "\n",
    "plt.xlim(xmin=SNRs[0], xmax=SNRs[-1])  # this line\n",
    "plt.grid(True, which='both')\n",
    "plt.savefig('result_bler_block_length_1000_snr0.png')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9ce788d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.279079,
   "end_time": "2024-12-26T17:20:47.345409",
   "environment_variables": {},
   "exception": true,
   "input_path": "turbo-nn/reports/benchmark.ipynb",
   "output_path": "turbo-nn/reports/benchmark_output.ipynb",
   "parameters": {},
   "start_time": "2024-12-26T17:20:43.066330",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}